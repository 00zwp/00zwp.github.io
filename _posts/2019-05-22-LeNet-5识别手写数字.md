---
layout:     post
title:      LeNet-5识别手写数字
subtitle:   ""
date:       2019-05-22
author:     tianhaoo
header-img: img/post-bg/39.jpg
catalog: true
tags:
  - python
  - tensorflow
  - 神经网络
---


## 卷积神经网络

### 特点

#### 局部连接

CNN利用了人类视觉的局部感受野的特性。所谓局部感受野，也就是说他们发现视觉神经元并不会对眼睛看到的所有图像有反应，而仅仅处理某个有限的区域。 这一事实，一定程度上启发了CNN的结构设计。而且，这也符合我们的一般认知：人对外界的认知一般是从局部到全局的，而图像的空间联系也是局部的像素联系较为紧密，而距离较远的像素相关性则较弱。

#### 权值共享

由于图片的像素实在太多了，为了减少神经元的个数，加快计算，防止过拟合，CNN采用参数共享的方法。参数共享就是让同一个隐含层中的神经元参数都相同。

### 构成

#### 卷积层

在前馈神经网络中我们训练w和b，在卷积神经网络中我们训练一个个的卷积核（滤波器），用来提取特征，多个卷积核帮助我们判断图片的类别。

#### 池化层（下采样）

池化（Pooling）是卷积神经网络中另一个重要的概念，它实际上是一种形式的降采样。有多种不同形式的非线性池化函数，而其中“最大池化（Max pooling）”是最为常见的。它是将输入的图像划分为若干个矩形区域，对每个子区域输出最大值。直觉上，这种机制能够有效地原因在于，在发现一个特征之后，它的精确位置远不及它和其他特征的相对位置的关系重要。池化层会不断地减小数据的空间大小，因此参数的数量和计算量也会下降，这在一定程度上也控制了过拟合。通常来说，CNN的卷积层之间都会周期性地插入池化层。

池化层通常会分别作用于每个输入的特征并减小其大小。当前最常用形式的池化层是每隔2个元素从图像划分出2 * 2的区块，然后对每个区块中的4个数取最大值。这将会减少75%的数据量。

#### 激活函数与损失函数

神经网络中都有这两个函数，不再赘述。

## LeNet-5

手写字体识别模型LeNet-5诞生于1994年，是最早的卷积神经网络之一。LeNet5通过巧妙的设计，利用卷积、参数共享、池化等操作提取特征，避免了大量的计算成本，最后再使用全连接神经网络进行分类识别，这个网络也是最近大量神经网络架构的起点。

![LeNet5](/img/20190522/1.png) 


### C1层(卷积层)

C1层有6个28x28的特征图组成，每个特征图中的任一个元素与该层的输入中一个5x5的区域相连接。

### S2层(池化层)

S2层是一个池化层，由6个14x14的特征图组成，每一个特征图中元素都与C1层中对应的特征图中一个2x2的相邻区域相连。这里的池化操作为：将4个输入相加，乘上一个可训练的系数，加上一个可训练的偏置，最后经过一个Sigmoid函数。 

### C3层(卷积层)

C3层由16个10x10的特征图组成，与C1的最大区别是，这里每个特征图中的元素会与S2层中若干个特征图中处于相同位置的5x5的区域相连。

### S4层(池化层)

S4层是一个池化层，他由16个5x5的特征图构成，其操作和S2层相同。其共有32和可训练的参数和2000个连接。 

### C5层(卷积层)

C5是一个类似C3的卷积层，由120个1x1的特征图组成。但是，与C3层不同的是，这里的连接是一种“全”连接，即每个特征图中的元素与S4层中每个特征图都连接。 



### F6层(全连接层)

F6就是一个简单的全连接层，它由84个神经元构成。和传统的全连接一样每个神经元将C5层中的特征图的值乘上相应的权重并相加，再加上对应的偏置再经过tanh激活函数。 

### 输出层

输出层由10个欧几里得径向基函数核(Euclidean Radial Basis Function, RBF)构成，每个核对应0-9中的一个类别。输出值最小的那个核对应的i就是这个模型识别出来的数字。




[参考 http://yann.lecun.com/exdb/publis/pdf/lecun-01a.pdf](http://yann.lecun.com/exdb/publis/pdf/lecun-01a.pdf)

[参考 https://www.charleychai.com/blogs/2018/ai/NN/lenet.html](https://www.charleychai.com/blogs/2018/ai/NN/lenet.html)