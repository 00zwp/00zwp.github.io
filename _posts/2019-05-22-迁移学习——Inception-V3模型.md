---
layout:     post
title:      迁移学习——Inception-V3模型
subtitle:   ""
date:       2019-05-22
author:     tianhaoo
header-img: img/post-bg/40.jpg
catalog: true
tags:
  - python
  - tensorflow
  - 神经网络
---

## CNN结构演化

![CNN all](/img/20190522/2.jpg) 

Inception 网络是 CNN 分类器发展史上一个重要的里程碑。在 Inception 出现之前，大部分流行 CNN 仅仅是把卷积层堆叠得越来越多，使网络越来越深，以此希望能够得到更好的性能。

例如第一个得到广泛关注的 AlexNet，它本质上就是扩展 LeNet 的深度，并应用一些 ReLU、Dropout 等技巧。AlexNet 有 5 个卷积层和 3 个最大池化层，它可分为上下两个完全相同的分支，这两个分支在第三个卷积层和全连接层上可以相互交换信息。与 Inception 同年提出的优秀网络还有 VGG-Net，它相比于 AlexNet 有更小的卷积核和更深的层级。

VGG-Net 的泛化性能非常好，常用于图像特征的抽取目标检测候选框生成等。VGG 最大的问题就在于参数数量，VGG-19 基本上是参数量最多的卷积网络架构。这一问题也是第一次提出 Inception 结构的 GoogLeNet 所重点关注的，它没有如同 VGG-Net 那样大量使用全连接网络，因此参数量非常小。

GoogLeNet 最大的特点就是使用了 Inception 模块，它的目的是设计一种具有优良局部拓扑结构的网络，即对输入图像并行地执行多个卷积运算或池化操作，并将所有输出结果拼接为一个非常深的特征图。因为 1*1、3*3 或 5*5 等不同的卷积运算与池化操作可以获得输入图像的不同信息，并行处理这些运算并结合所有结果将获得更好的图像表征。

## 迁移学习

### 什么是迁移学习

在深度学习中，所谓的迁移学习是将一个问题A上训练好的模型通过简单的调整使其适应一个新的问题B。在实际使用中，往往是完成问题A的训练出的模型有更完善的数据，而问题B的数据量偏小。而调整的过程根据现实情况决定，可以选择保留前几层卷积层的权重，以保留低级特征的提取；也可以保留全部的模型，只根据新的任务改变其fc层。

### 迁移学习能做什么 

那么对于不同的任务，为什么不同的模型间可以做迁移呢？上面提到了，被迁移的模型往往是使用大量样本训练出来的，比如Google提供的Inception V3网络模型使用ImageNet数据集训练，而ImageNet中有120万标注图片，然后在实际应用中，很难收集到如此多的样本数据。而且收集的过程需要消耗大量的人力无力（其实深度学习解决实际问题时，最好费时间的往往不是训练的过程，而是数据标记的过程），所以一般情况下来说，问题B的数据量是较少的。

所以，同样一个模型在使用大样本很好的解决了问题A，那么有理由相信该模型中训练处的权重参数能够能够很好的完成特征提取任务（最起码前几层是这样），所以既然已经有了这样一个模型，那就拿过来用吧。

所以迁移学习具有如下优势：

* 更短的训练时间
* 更快的收敛速度
* 更精准的权重参数。

但是一般情况下如果任务B的数据量是足够的，那么迁移来的模型效果会不如训练的好，但是此时起码可以将底层的权重参数作为初始值来重新训练。

### 利用inception-V3模型进行迁移学习

![CNN all](/img/20190522/3.png)




[参考 https://zhuanlan.zhihu.com/p/30756181](https://zhuanlan.zhihu.com/p/30756181)

