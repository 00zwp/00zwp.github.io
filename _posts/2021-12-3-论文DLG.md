---
layout: post
title: Deep Leakage from Gradients
description: ""
subtitle: ""
data: 2021-12-3
author: Fat-Pman 
header-img: img/post-bg/15.jpg
catalog: true
tags: Papers
---

#### Deep Leakage from Gradients

#### Abstract
交换梯度是现代多节点机器学习系统(如分布式训练、协作学习)中广泛使用的一种方法。长期以来，人们认为梯度共享是安全的，即梯度交换不会泄露训练数据。然而，我们证明了从公开共享的梯度中获得私人的训练数据是可能的。我们将这种泄漏称为梯度深度泄漏，并通过经验验证了其在计算机视觉和自然语言处理任务中的有效性。实验结果表明，我们的攻击比以前的方法强得多:对图像的恢复是像素精确的，对文本的标记匹配。因此，我们希望提高人们的意识，重新思考梯度的安全性。我们还讨论了几种可能的防止这种深泄漏的策略。在不改变训练设置的情况下，最有效的防御方法是梯度剪枝.

#### Introduction

分布式训练成为在大规模数据集上加速训练的必要条件。在分布式学习系统中，计算在每个worker上并行执行，并通过交换梯度(参数服务器[15,23]和all-reduce[3,30])来同步。计算的分布自然导致了数据的分裂:每个客户端都有自己的训练数据，在训练过程中只交流梯度(表示训练集从未离开本地机器)。它允许使用来自多个数据源的数据来训练模型，而不需要将它们集中起来。该方案被称为协作学习，广泛应用于训练集包含私有信息的情况[18,20]。例如，多家医院联合培训一个模型，而不共享其患者的医疗数据[17,26]。分布式训练和协作学习在大规模机器学习任务中得到了广泛的应用。然而，“梯度共享”方案是否保护了每个参与者的训练数据集的隐私?在大多数情况下，人们认为梯度共享是安全的，不会暴露训练数据。最近的一些研究表明，梯度揭示了训练数据的一些属性，例如属性分类器[27](是否具有某些属性的样本在批中)，以及使用生成对抗网络生成看起来类似于训练图像的图片[9,13,27]。这里我们考虑一个更有挑战性的情况:我们能否完全从梯度中窃取训练数据?正式地说，给定一个机器学习模型F()和它的权值W，如果我们有一个∇W w.r.t的梯度:一对输入和标签，我们可以反向获得训练数据吗?传统观点认为答案是否定的，但我们证明了这实际上是可能的.

深泄漏对多节点机器学习系统提出了严峻的挑战。在我们的工作中，基本梯度共享方案在保护训练数据隐私方面并不总是可靠的。在集中式分布式训练中(图1a)，参数服务器通常不存储任何训练数据，可以窃取所有参与者的本地训练数据。对于去中心化分布式培训(图1b)，更糟糕的是，任何参与者都可以窃取其邻居的私人培训数据。为了防止深度泄漏，我们提出了三种防御策略:梯度摄动、低精度和梯度压缩。对于梯度扰动，我们发现比例高于10−2的高斯噪声和拉普拉斯噪声都是很好的防御方法。而一半精度不能保护，梯度压缩成功地防御攻击，修剪的梯度超过20%